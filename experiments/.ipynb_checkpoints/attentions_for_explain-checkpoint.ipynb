{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention for explaining text generation\n",
    "We would like to know if we can used the attention of a model to explain where he gets his informations to generate an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c35506087442b88866273b7e01fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linagora/anaconda3/envs/rag2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/linagora/anaconda3/envs/rag2/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "path_model = \"openchat/openchat-3.5-0106\" #\"microsoft/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(path_model, torch_dtype=\"auto\", trust_remote_code=True, attn_implementation=\"eager\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_model, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking 3 differents questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_P7Eb2Z-W2eZ",
    "outputId": "0393df72-7811-43cc-fe10-3d3f155e7ef1"
   },
   "outputs": [],
   "source": [
    "question = \"What is the most common tree in south of England? Who is the first emperor of france? Who is the last king of France?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=80,return_dict_in_generate=True, output_attentions= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "U96-gWkxXXXG"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.batch_decode(outputs[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ycHsjODW9oX",
    "outputId": "4dcd2584-939c-49d4-ca41-2041c580e68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the most common tree in south of England? Who is the first emperor of france? Who is the last king of France? What is the most common tree in south of England? The most common tree in the south of England is the Oak tree. The first emperor of France was Napoleon Bonaparte. The last king of France was Louis XVI.\n",
      "\n",
      "## What is the\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectations\n",
    "We have a structured answer with 3 sentences were each one answer 1 questions\n",
    "So we except that the attention for generating each answer is highly correlated to question wich is refering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerateDecoderOnlyOutput\n",
    "import numpy as np\n",
    "\n",
    "class Attentions():\n",
    "    \"\"\"\n",
    "    Class to manage attention given by the outputs of our model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, outputs: GenerateDecoderOnlyOutput, heads: list[int]):\n",
    "        self.n = outputs[0].shape[1]\n",
    "        self.attentions = self.get_attentions(outputs, heads)\n",
    "\n",
    "    def get_attentions(self, outputs: GenerateDecoderOnlyOutput, heads: list[int]) -> np.array:\n",
    "        \"\"\"\n",
    "        Transform the attention tensor to a nice numpy array \n",
    "        where the first argument is the generated token\n",
    "        and the second one is the attention on one of the previous token.\n",
    "\n",
    "        Ex attentions[i, j] is the attention used on j for generating i.\n",
    "\n",
    "        *** heads : is wich heads we looks at (sum is made over all those heads)\n",
    "        \"\"\"\n",
    "        layer = 0 #The layer where we look at the attentionn\n",
    "        n = outputs[0].shape[1]\n",
    "        result = np.zeros((n,n))\n",
    "        attentions = outputs.attentions\n",
    "        h = len(attentions[0])\n",
    "        p = len(attentions)\n",
    "        s = torch.sum(attentions[0][layer].squeeze(), dim =0)\n",
    "        for i in range(n-p):\n",
    "            for j in range(i+1):\n",
    "                result[i+1][j] = s[i][j]      \n",
    "        for i in range(1,p):\n",
    "            s = torch.sum(attentions[i][layer].squeeze()[heads], dim =0)\n",
    "            for j in range(i + n - p):\n",
    "                result[i+n-p][j] = np.float64(s[j])\n",
    "        return result/h\n",
    "\n",
    "    def get_attentions_for_seq(self, seq :np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Return the attention for all token to a given sequence\n",
    "        So as input the generated token\n",
    "        \"\"\"\n",
    "        return sum(self.attentions[seq])/len(seq)\n",
    "        \n",
    "    def get_attention_from_seq(self, att_seq: np.array, seq :np.array) ->np.float64:\n",
    "        \"\"\"\n",
    "        Return the global attention from att_seq to seq\n",
    "        Seq are the input token that we are interested in\n",
    "        \"\"\"\n",
    "        return sum(att_seq[seq])/len(seq)\n",
    "\n",
    "    def attention_from_seq_to_seq(self, from_seq: np.array, for_seq: np.array) -> np.float64:\n",
    "        \"\"\"\n",
    "        For_seq is the generated tokens\n",
    "        From seq is the based tokens\n",
    "        \"\"\"\n",
    "        return self.get_attention_from_seq(self.get_attentions_for_seq(for_seq), from_seq)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>(0)What(1)is(2)the(3)most(4)common(5)tree(6)in(7)south(8)of(9)England(10)?(11)Who(12)is(13)the(14)first(15)emperor(16)of(17)fr(18)ance(19)?(20)Who(21)is(22)the(23)last(24)king(25)of(26)France(27)?(28)What(29)is(30)the(31)most(32)common(33)tree(34)in(35)south(36)of(37)England(38)?(39)The(40)most(41)common(42)tree(43)in(44)the(45)south(46)of(47)England(48)is(49)the(50)Oak(51)tree(52).(53)The(54)first(55)emperor(56)of(57)France(58)was(59)Napoleon(60)Bon(61)ap(62)arte(63).(64)The(65)last(66)king(67)of(68)France(69)was(70)Louis(71)XVI(72).(73)\n",
      "(74)\n",
      "(75)##(76)What(77)is(78)the(79)\n"
     ]
    }
   ],
   "source": [
    "tensor_outputs = outputs[0][0]\n",
    "def print_token(i):\n",
    "   return tokenizer.decode(tensor_outputs[i])\n",
    "    \n",
    "def print_token_nb(outputs):\n",
    "    s= ''\n",
    "    for i in range(len(outputs[0][0])):\n",
    "        s +=print_token(i) +'('+str(i)+')'\n",
    "    print(s)\n",
    "\n",
    "print_token_nb(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_roi = np.array(range(0,11))\n",
    "input_emperor = np.array(range(11,20))\n",
    "input_tree = np.array(range(20,28))\n",
    "\n",
    "output_roi = np.array(range(40,53))\n",
    "output_emperor = np.array(range(54,64))\n",
    "output_tree = np.array(range(64,73))\n",
    "\n",
    "inputs_seq = [input_roi,input_emperor,input_tree]\n",
    "outputs_seq = [output_roi,output_emperor,output_tree]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0 0 0]\n",
      "1\n",
      "[0 0 0]\n",
      "2\n",
      "[0 0 0]\n",
      "3\n",
      "[0 0 0]\n",
      "4\n",
      "[0 0 0]\n",
      "5\n",
      "[0 0 0]\n",
      "6\n",
      "[0 0 0]\n",
      "7\n",
      "[0 0 0]\n",
      "8\n",
      "[0 0 0]\n",
      "9\n",
      "[0 0 0]\n",
      "10\n",
      "[0 0 0]\n",
      "11\n",
      "[0 0 0]\n",
      "12\n",
      "[0 0 0]\n",
      "13\n",
      "[0 0 0]\n",
      "14\n",
      "[0 0 0]\n",
      "15\n",
      "[0 0 0]\n",
      "16\n",
      "[1 1 1]\n",
      "17\n",
      "[0 0 0]\n",
      "18\n",
      "[0 0 0]\n",
      "19\n",
      "[2 0 0]\n",
      "20\n",
      "[0 0 0]\n",
      "21\n",
      "[0 0 0]\n",
      "22\n",
      "[0 0 0]\n",
      "23\n",
      "[0 0 0]\n",
      "24\n",
      "[0 0 0]\n",
      "25\n",
      "[0 0 0]\n",
      "26\n",
      "[2 0 0]\n",
      "27\n",
      "[0 0 0]\n",
      "28\n",
      "[0 0 0]\n",
      "29\n",
      "[0 0 0]\n",
      "30\n",
      "[2 0 0]\n",
      "31\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for k in range(32):\n",
    "    heads = [k]\n",
    "    att = Attentions(outputs, heads)\n",
    "    correspondance = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            correspondance[i,j]= att.attention_from_seq_to_seq(inputs_seq[i], outputs_seq[j])\n",
    "    print(k)\n",
    "    print(np.argmax(correspondance*100, axis =0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Nothing came up on this we cant clearly relate on attention to see what have been used for generation the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag2",
   "language": "python",
   "name": "rag2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
